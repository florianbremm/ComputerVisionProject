{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from spyder_kernels.customize.spydercustomize import cell_count\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "cfae6f03ac3974b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ------------  CONFIG  -------------------\n",
    "base_path = '/scratch/cv-course-group-5/data/dataset_jpg'\n",
    "TARGET_SIZE = 224\n",
    "src_root   = Path(base_path + '/dataset')\n",
    "dst_root   = Path(base_path + '/preprocessed_dataset')\n",
    "lmdb_path = Path(base_path + '/lmdb')\n",
    "anno_file  = Path(base_path + '/dataset/annotations.json')"
   ],
   "id": "6b7648e5a5258a74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "annos_dict = json.loads(anno_file.read_text())\n",
    "\n",
    "\n",
    "images = annos_dict.get('images', [])\n",
    "\n",
    "images"
   ],
   "id": "312121bc72195281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "json_path = Path('train_test_split.json')\n",
    "\n",
    "# Load the file\n",
    "with open(json_path, 'r') as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "# Access the train and test entries\n",
    "train_list = split_data.get(\"train\", [])\n",
    "train_list"
   ],
   "id": "592616288e952591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cells_by_video = {}\n",
    "\n",
    "for image in images:\n",
    "    if next(filter(lambda train_video: train_video['id'] == image['video_id'], train_list), False):\n",
    "        if image['video_id'] not in cells_by_video:\n",
    "            cells_by_video[image['video_id']] = {'cells': 0, 'dead_cells': 0, 'cells_alive': 0}\n",
    "        cells_by_video[image['video_id']]['cells'] += image['cells_alive'] + image['dead_cells']\n",
    "        cells_by_video[image['video_id']]['dead_cells'] += image['dead_cells']\n",
    "        cells_by_video[image['video_id']]['cells_alive'] += image['cells_alive']\n",
    "\n",
    "cells_by_video"
   ],
   "id": "11af6b2e196234e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(4)\n",
    "\n",
    "ax[0].hist([video['cells'] for video in cells_by_video.values()], bins=100)\n",
    "ax[0].set_xlabel('cells')\n",
    "ax[0].set_ylabel('video count')\n",
    "ax[1].hist([video['dead_cells'] for video in cells_by_video.values()], bins=100)\n",
    "ax[1].set_xlabel('dead cells')\n",
    "ax[1].set_ylabel('video count')\n",
    "ax[2].hist([video['cells_alive'] for video in cells_by_video.values()], bins=100)\n",
    "ax[2].set_xlabel('alive cells')\n",
    "ax[2].set_ylabel('video count')\n",
    "\n",
    "fractions_of_dead_cells = np.array([video['dead_cells'] / video['cells'] for video in cells_by_video.values()])\n",
    "ax[3].hist(fractions_of_dead_cells, bins=100)\n",
    "ax[3].axvline(np.mean(fractions_of_dead_cells), color='r')\n",
    "ax[3].axvline(np.median(fractions_of_dead_cells), color='b')\n",
    "ax[3].set_xlabel('fraction of dead cells')\n",
    "ax[3].set_ylabel('video count')\n",
    "\n",
    "fig.set_size_inches(20, 30)\n",
    "fig.show()"
   ],
   "id": "927383dae134084d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(f'avg fraction of dead cells: {np.mean(fractions_of_dead_cells)}: ')\n",
    "print(f'median fraction of dead cells: {np.median(fractions_of_dead_cells)}')"
   ],
   "id": "728b15c1c4eacad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "cell_count = 0\n",
    "reduced_videos = []\n",
    "CELL_THRESHOLD = 100000\n",
    "random.seed(42)\n",
    "\n",
    "while cell_count < CELL_THRESHOLD:\n",
    "    next = train_list.pop(random.randint(0, len(train_list) - 1))\n",
    "    reduced_videos.append(next)\n",
    "    cell_count += cells_by_video[next['id']]['cells']\n",
    "\n",
    "json.dump(reduced_videos, open('reduced_videos.json', 'w'))"
   ],
   "id": "d7f1e6e45d60e10"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
